{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to consume Amazon Bedrock models like Amazon Nova, Anthropic Claude and Amazon Titan Models via SAP GenAI hub\n",
    "## Part 1 Use AI Core REST APIs\n",
    "\n",
    "Use this notebook to invoke the AI Core REST APIs to send your payloads into LLMs hosted on SAP GenAI hub. The documentation for the APIs are provided [here](https://api.sap.com/api/AI_CORE_API/resource/Deployment) and [here](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/consume-generative-ai-models-using-sap-ai-core#aws-bedrock)\n",
    "\n",
    "The objective of this notebook is to help you understand how to consume Bedrock models via SAP GenAI hub for your GenAI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load your SAP AI Core credentials\n",
    "\n",
    "First, you should have your AI core creadentials in your ~/.aicore/config.json. If you do not have one yet, get it by creating a service key from [here](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/create-service-key). In this notebook, we will read and consume the data from this location into temporary environment variables.\n",
    "\n",
    "A sample config.json is below:\n",
    "\n",
    "```sh\n",
    "$ cat ~/.aicore/config.json  \n",
    "{\n",
    "  \"AICORE_AUTH_URL\": \"<>.authentication.us10.hana.ondemand.com\",\n",
    "  \"AICORE_CLIENT_ID\": \"sb-b...64\",\n",
    "  \"AICORE_CLIENT_SECRET\": \"21...xc=\",\n",
    "  \"AICORE_RESOURCE_GROUP\": \"default\",\n",
    "  \"AICORE_BASE_URL\": \"https://api.ai.prod.<>.ondemand.com\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def load_config():\n",
    "    with open(os.path.expanduser('~/.aicore/config.json')) as f:\n",
    "        config = json.load(f)\n",
    "    for key, value in config.items():\n",
    "        os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Utility functions\n",
    "Creating a utility function to send API calls to AI Core instance repeatedly. You will need to install requests library \n",
    "```sh\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from util.logging import initLogger\n",
    "\n",
    "TIME_RETRY_API_CALL = 20\n",
    "TIMEOUT_API_CALL = 3600\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "initLogger()\n",
    "\n",
    "\n",
    "# Function to call a rest API\n",
    "def call_api(\n",
    "    type: str, url: str, headers: dict, data: dict = None, message: str = None\n",
    "):\n",
    "    timeNeeded = 0\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        try:\n",
    "            r = None\n",
    "            # Send the request to retrieve the access token\n",
    "            if type == \"POST\":\n",
    "                r = requests.post(url=url, headers=headers, data=data)\n",
    "            elif type == \"GET\":\n",
    "                r = requests.get(url=url, headers=headers)\n",
    "            # if the response is OK, return the JSON response\n",
    "            if r.ok is True:\n",
    "                log.success(f\"{message}\")\n",
    "                return r.json()\n",
    "            else:\n",
    "                log.info(\n",
    "                    f\"response ({message}): {r.status_code} ({r.reason}): {r.text}\"\n",
    "                )\n",
    "                log.warning(\n",
    "                    f\"Could not {message}! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "                )\n",
    "                time.sleep(TIME_RETRY_API_CALL)\n",
    "                timeNeeded += TIME_RETRY_API_CALL\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log.warning(str(e))\n",
    "            log.error(f\"Could not {message}! Exiting...\")\n",
    "            sys.exit(1)\n",
    "    log.error(f\"Could not {message} after {TIMEOUT_API_CALL} seconds! Exiting...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a template to hold the deploymentURL and other imporatant parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class AiCoreMetadataJsonEncoder(JSONEncoder):\n",
    "    def default(self, o):\n",
    "        return o.__dict__\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AiCoreMetadataDefinition:\n",
    "    authUrl: str\n",
    "    clientId: str\n",
    "    clientSecret: str\n",
    "    apiBase: str\n",
    "    resourceGroup: str\n",
    "    targetAiCoreModel: str\n",
    "    apiAccessToken: str\n",
    "    deploymentUrl : str\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataClass that will initiate with the environment variables that have the AI Core credentials as well as the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AiCoreMetadata(AiCoreMetadataDefinition):\n",
    "    def __init__(self):\n",
    "\n",
    "        load_config()\n",
    "\n",
    "        self.authUrl = os.environ.get(\"AICORE_AUTH_URL\")\n",
    "        self.clientId = os.environ.get(\"AICORE_CLIENT_ID\")\n",
    "        self.clientSecret = os.environ.get(\"AICORE_CLIENT_SECRET\")\n",
    "        self.resourceGroup = os.environ.get(\"AICORE_RESOURCE_GROUP\")\n",
    "        self.apiBase = os.environ.get(\"AICORE_BASE_URL\")\n",
    "        self.targetAiCoreModel = os.environ.get(\"TARGET_AI_CORE_MODEL\")\n",
    "        self.apiAccessToken = get_api_access_token(self)\n",
    "        self.deploymentUrl = get_deployment_details_for_model(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a utility function that will get the API Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_access_token(aiCoreMetadata: AiCoreMetadataDefinition) -> str:\n",
    "    clientId = aiCoreMetadata.clientId\n",
    "    clientSecret = aiCoreMetadata.clientSecret\n",
    "    authUrl = aiCoreMetadata.authUrl\n",
    "\n",
    "    # Create the authorization string\n",
    "    authorizationString = f\"{clientId}:{clientSecret}\"\n",
    "    # Encode the authorization string\n",
    "    byte_data = authorizationString.encode(\"utf-8\")\n",
    "    # Base64 encode the byte data\n",
    "    clientSecretBase64 = base64.b64encode(byte_data).decode(\"utf-8\")\n",
    "\n",
    "    # Create the URL to retrieve the access token\n",
    "    aiCoreLocation = f\"{authUrl}/oauth/token?grant_type=client_credentials\"\n",
    "    # Create the headers for the request\n",
    "    headers = {\"Authorization\": f\"Basic {clientSecretBase64}\"}\n",
    "\n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        None,\n",
    "        \"retrieve access token from AI Core system\",\n",
    "    )\n",
    "    # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the API Access token, lets get the deployment URL that corresponds to the Model you want to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details_for_model(aiCoreMetadata: AiCoreMetadataDefinition):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to get the deployment \n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    allDeploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for model id {aiCoreMetadata.targetAiCoreModel}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to get the list of deployments\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        # json_response = json.dumps(response, indent=2)\n",
    "        # log.check(\n",
    "        #         f\"API response from retrieveing deployment details for model id {aiCoreMetadata.targetAiCoreModel}:\\n{json_response}\"\n",
    "        # )\n",
    "\n",
    "        allDeploymentDetails = response\n",
    "    \n",
    "        for resource in allDeploymentDetails[\"resources\"]:\n",
    "            if resource[\"scenarioId\"] == \"foundation-models\":\n",
    "                model_name = resource[\"details\"][\"resources\"][\"backend_details\"][\"model\"][\"name\"]\n",
    "                if model_name == aiCoreMetadata.targetAiCoreModel:\n",
    "                    return resource[\"deploymentUrl\"]\n",
    "        \n",
    "        log.warning(\n",
    "                f\"Could not {message} Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "        time.sleep(TIME_RETRY_API_CALL)\n",
    "        timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{aiCoreMetadata.targetAiCoreModel}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another utility function to get deployment details in case deployment ID is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the deployment URL from the AI Core system metadata\n",
    "def get_deployment_details(aiCoreMetadata: AiCoreMetadataDefinition, deploymenId: str):\n",
    "    apiBase = aiCoreMetadata.apiBase\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    resourceGroup = aiCoreMetadata.resourceGroup\n",
    "\n",
    "    # Create the URL to create the configuration\n",
    "    aiCoreLocation = f\"{apiBase}/v2/lm/deployments/{deploymenId}\"\n",
    "    # Create the headers for the request\n",
    "    headers = {}\n",
    "    headers[\"AI-Resource-Group\"] = resourceGroup\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    deploymentDetails = None\n",
    "\n",
    "    timeNeeded = 0\n",
    "    message = f\"retrieve deployment details for deployment id {deploymenId}\"\n",
    "    while timeNeeded < TIMEOUT_API_CALL:\n",
    "        # Send the request to create the deployment\n",
    "        response = call_api(\"GET\", aiCoreLocation, headers, None, message)\n",
    "        # json_response = json.dumps(response, indent=2)\n",
    "\n",
    "        deploymentDetails = response\n",
    "        deploymentUrl = deploymentDetails[\"deploymentUrl\"]\n",
    "        if deploymentUrl != \"\":\n",
    "            log.success(f\"AI Core deployment id '{deploymenId}' is now accessible!\")\n",
    "            return deploymentDetails\n",
    "        else:\n",
    "            log.warning(\n",
    "                f\"Could not {message} (deployment not finished)! Re-trying in {TIME_RETRY_API_CALL} seconds...\"\n",
    "            )\n",
    "\n",
    "            time.sleep(TIME_RETRY_API_CALL)\n",
    "            timeNeeded += TIME_RETRY_API_CALL\n",
    "\n",
    "    log.error(\n",
    "        f\"Could not retrieve deployment details for id '{deploymenId}'! Exiting...\"\n",
    "    )\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our class. This step will populate the internal environment variables, execute the access token get utility function and also get the deployment URL for the model specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the utility function to send your input prompt to GenAI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the available AI models from the AI Core system\n",
    "def invoke(aiCoreMetadata: AiCoreMetadataDefinition, \n",
    "           data) -> str:\n",
    "\n",
    "    token = aiCoreMetadata.apiAccessToken\n",
    "    deploymentUrl = aiCoreMetadata.deploymentUrl\n",
    "    \n",
    "    # Determine the endpoint based on the target AI Core model\n",
    "    nova_models = [\"amazon--nova-pro\", \"amazon--nova-micro\", \"amazon--nova-lite\"]\n",
    "    aiCoreLocation = f\"{deploymentUrl}/converse\" if aiCoreMetadata.targetAiCoreModel in nova_models else f\"{deploymentUrl}/invoke\"\n",
    "    \n",
    "    # Create the headers for the request\n",
    "    headers = {\n",
    "        \"AI-Resource-Group\": aiCoreMetadata.resourceGroup,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "    }\n",
    "    \n",
    "    response = call_api(\n",
    "        \"POST\",\n",
    "        aiCoreLocation,\n",
    "        headers,\n",
    "        json.dumps(data),\n",
    "        \"sending invoke\",\n",
    "    )\n",
    "    json_response = json.dumps(response, indent=2)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Send the prompt to the GenAI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Amazon Nova models\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--nova-pro\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--nova-lite\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--nova-micro\"\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    { \"text\": \"You should respond to all messages in german\" }\n",
    "]\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"What is the capital of United States?\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 150, \"temperature\": 0.7}\n",
    "\n",
    "data = {\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and save the metadata for the AI Core \n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a payload for Anthropic models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Anthropic Claude models\n",
    "\n",
    "# Multimodal models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3.5-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-opus\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-sonnet\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"anthropic--claude-3-haiku\"\n",
    "\n",
    "data = {}\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, What is the capital of United States?\"\n",
    "        }\n",
    "    ]\n",
    "data[\"anthropic_version\"] = \"bedrock-2023-05-31\"\n",
    "data[\"max_tokens\"] = 1000\n",
    "data[\"messages\"] = messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Amazon Titan Text Models\n",
    "\n",
    "#Text based models:\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-lite\"\n",
    "# os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-text-express\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "data = {\n",
    "    \"inputText\": \"What is the capital of United States?\",\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 1000,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Amazon Titan Embedding Models\n",
    "os.environ[\"TARGET_AI_CORE_MODEL\"] = \"amazon--titan-embed-text\"\n",
    "\n",
    "data = {}\n",
    "\n",
    "data = {\n",
    "    \"inputText\": \"What is the capital of United States?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metadata for the AI Core system\n",
    "ai_core_metadata = AiCoreMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally get a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke(aiCoreMetadata= ai_core_metadata, \n",
    "       data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Use SAP GenAI Hub SDK\n",
    "Reference code [here](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/gen_ai_hub.html).\n",
    "\n",
    "Run below if not installed\n"
   ]
  },
  
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"generative-ai-hub-sdk[all]==4.4.3\" \"boto3==1.35.27\" \"langchain==0.3.20\" \"langgraph==0.3.20\""
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.amazon.clients import Session\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = Session().client(model_name=\"amazon--nova-lite\")\n",
    "# bedrock = Session().client(model_name=\"anthropic--claude-3-haiku\")\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "    {\"text\": \"You should respond to all messages in german\"}\n",
    "]\n",
    "\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": \"What is the capital of United States?\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 150, \"temperature\": 0.7}\n",
    "\n",
    "# Get the response from the model\n",
    "response = bedrock.converse(\n",
    "    messages=message_list,\n",
    "    system=system_list,\n",
    "    inferenceConfig=inf_params\n",
    ")\n",
    "\n",
    "# Extract and print the assistant's response\n",
    "if 'output' in response and 'message' in response['output']:\n",
    "    assistant_message = response['output']['message']['content'][0]['text']\n",
    "    print(\"Response:\")\n",
    "    print(assistant_message)\n",
    "else:\n",
    "    print(\"No valid response received.\")\n",
    "\n",
    "# Extract and print additional details: stopReason, usage, and metrics\n",
    "if 'stopReason' in response:\n",
    "    print(\"\\nStop Reason:\")\n",
    "    print(response['stopReason'])\n",
    "\n",
    "if 'usage' in response:\n",
    "    print(\"\\nUsage Details:\")\n",
    "    for key, value in response['usage'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "if 'metrics' in response:\n",
    "    print(\"\\nMetrics:\")\n",
    "    for key, value in response['metrics'].items():\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
